<!doctype html><html lang=ja><head><title>CartoonGAN: Generative Adversarial Networks for Photo Cartoonization // dev.touyu.me</title><meta charset=utf-8><meta name=generator content="Hugo 0.61.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="touyu"><meta name=description content><link rel=stylesheet href=https://dev.touyu.me/css/main.min.8b9e00c2aba58c0f47804d6644e5a511733d481c23a2cd6fb0541b5fe30449b2.css><meta name=twitter:title content="CartoonGAN: Generative Adversarial Networks for Photo Cartoonization"><meta name=twitter:description content="Paper https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf
Abstruct 既存手法（Style Transfer, CycleGAN)では、実写画像のアニメ調変換においては、十分な結果を得られていない。 VGGのFeature Mapを用いたContent Lossと、明確なエッジを生成するためのAdversarial Lossの２つの効果的な損失関数を提案。
Architecture Loss function $$ \mathcal{L}(G, D)=\mathcal{L}_{a d v}(G, D)+\omega \mathcal{L}_{c o n}(G, D) \tag{2} $$
StyleとContentのバランスが良い状態になるのは、$\omega=10$
Adversarial loss $$ \begin{aligned} \mathcal{L}_{a d v}(G, D) &=\mathbb{E}_{c_{i} \sim S_{d a t a}(c)}\left[\log D\left(c_{i}\right)\right] \\
&+\mathbb{E}_{e_{j} \sim S_{d a t a}(e)}\left[\log \left(1-D\left(e_{j}\right)\right)\right] \\
&+\mathbb{E}_{p_{k} \sim S_{d a t a}(p)}\left[\log \left(1-D\left(G\left(p_{k}\right)\right)\right)\right] \\
\end{aligned} \tag{3} $$
生成される画像のエッジを明確なものにするために、
 通常の訓練画像 → true エッジをぼかした訓練画像 → false 生成画像 → false  でDiscriminatorを学習させる"><meta property="og:title" content="CartoonGAN: Generative Adversarial Networks for Photo Cartoonization"><meta property="og:description" content="Paper https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf
Abstruct 既存手法（Style Transfer, CycleGAN)では、実写画像のアニメ調変換においては、十分な結果を得られていない。 VGGのFeature Mapを用いたContent Lossと、明確なエッジを生成するためのAdversarial Lossの２つの効果的な損失関数を提案。
Architecture Loss function $$ \mathcal{L}(G, D)=\mathcal{L}_{a d v}(G, D)+\omega \mathcal{L}_{c o n}(G, D) \tag{2} $$
StyleとContentのバランスが良い状態になるのは、$\omega=10$
Adversarial loss $$ \begin{aligned} \mathcal{L}_{a d v}(G, D) &=\mathbb{E}_{c_{i} \sim S_{d a t a}(c)}\left[\log D\left(c_{i}\right)\right] \\
&+\mathbb{E}_{e_{j} \sim S_{d a t a}(e)}\left[\log \left(1-D\left(e_{j}\right)\right)\right] \\
&+\mathbb{E}_{p_{k} \sim S_{d a t a}(p)}\left[\log \left(1-D\left(G\left(p_{k}\right)\right)\right)\right] \\
\end{aligned} \tag{3} $$
生成される画像のエッジを明確なものにするために、
 通常の訓練画像 → true エッジをぼかした訓練画像 → false 生成画像 → false  でDiscriminatorを学習させる"><meta property="og:type" content="article"><meta property="og:url" content="https://dev.touyu.me/posts/cartoongan/"><meta property="og:image" content="https://dev.touyu.me/og.png"><meta property="article:published_time" content="2020-10-11T22:18:19+09:00"><meta property="article:modified_time" content="2020-10-11T22:18:19+09:00"><link rel=stylesheet href=https://dev.touyu.me/css/custom.css><link href="https://fonts.googleapis.com/css?family=M+PLUS+1p" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body><main class=app-container><article class=post><header class=post-header><h1 class=post-title>CartoonGAN: Generative Adversarial Networks for Photo Cartoonization</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Oct 11, 2020</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>1 min read</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><a class=tag href=https://dev.touyu.me/tags/deep-learning/>deep-learning</a></div></div></header><div class=post-content><h1 id=paper>Paper</h1><p><a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf>https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf</a></p><h1 id=abstruct>Abstruct</h1><p>既存手法（Style Transfer, CycleGAN)では、実写画像のアニメ調変換においては、十分な結果を得られていない。
VGGのFeature Mapを用いたContent Lossと、明確なエッジを生成するためのAdversarial Lossの２つの効果的な損失関数を提案。</p><h1 id=architecture>Architecture</h1><p><img src=arch.jpg alt=arch></p><h2 id=loss-function>Loss function</h2><p>$$
\mathcal{L}(G, D)=\mathcal{L}_{a d v}(G, D)+\omega \mathcal{L}_{c o n}(G, D) \tag{2}
$$</p><p>StyleとContentのバランスが良い状態になるのは、$\omega=10$</p><h3 id=adversarial-loss>Adversarial loss</h3><p>$$
\begin{aligned}
\mathcal{L}_{a d v}(G, D) &=\mathbb{E}_{c_{i} \sim S_{d a t a}(c)}\left[\log D\left(c_{i}\right)\right] \\<br>&+\mathbb{E}_{e_{j} \sim S_{d a t a}(e)}\left[\log \left(1-D\left(e_{j}\right)\right)\right] \\<br>&+\mathbb{E}_{p_{k} \sim S_{d a t a}(p)}\left[\log \left(1-D\left(G\left(p_{k}\right)\right)\right)\right] \\<br>\end{aligned}
\tag{3}
$$</p><p>生成される画像のエッジを明確なものにするために、</p><ul><li>通常の訓練画像 → true</li><li>エッジをぼかした訓練画像 → false</li><li>生成画像 → false</li></ul><p>でDiscriminatorを学習させる</p><h3 id=content-loss>Content loss</h3><p>$$
\begin{array}{l}
\mathcal{L}_{\text {con}}(G, D)=\mathbb{E}_{p_{i} \sim S_{\text {data}}(p)}\left[\left|V G G_{l}\left(G\left(p_{i}\right)\right)-V G G_{l}\left(p_{i}\right)\right|_{1}\right]
\end{array}
\tag{4}
$$</p><p>VGGのFeature Map(Conv4_4)の差のL1スパース正則化で定式化。</p><p>多くの先行研究では、Content lossは、L2やMSEを用いることが多いが、L1を使うことで良い結果が得られた。</p><p><img src=loss.jpg alt=loss></p><h1 id=result>Result</h1><p><img src=result.jpg alt=result></p><p>Gatysらの結果のほうがよく見えるが、Style Transferは、1対1の変換しかできず、抽象的に画風を学習できない。
それに加えて、同じような場面のスタイル画像をユーザが選ばなければならない。</p></div><div class=post-footer></div></article></main></body></html>